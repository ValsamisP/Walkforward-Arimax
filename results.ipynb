{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_prep import load_prices\n",
    "from feature_engineering import build_features, make_nextday_target, add_lags\n",
    "from model_arimax import walk_forward_arimax, analyze_residuals\n",
    "from evaluation_visualization import  plot_detailed_strategy_window,plot_strategy_heatmap,plot_rolling_performance_metrics,enhanced_plot_cum_pnl,run_cfg,print_overall_metrics,plot_hitrate_timeline,plot_cum_pnl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from types import SimpleNamespace\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ASSET   = \"NVDA\"\n",
    "MARKET  = \"QQQ\"\n",
    "START   = \"2020-01-01\"\n",
    "END     = \"2025-12-31\"\n",
    "\n",
    "TRAIN_LEN = 504                 \n",
    "ORDER_GRID = ((1,0,0),(0,0,1),(1,0,1),(2,0,1),(2,0,2))\n",
    "EXOG_SETS = [\n",
    "    [\"mkt_ret_l1\", \"macd_hist_l1\"],\n",
    "    [\"mkt_ret_l1\",\"macd_hist_l1\",\"macd_hist_l2\",\"vol_z_l1\"]\n",
    "]\n",
    "TARGET    = \"y_next\"\n",
    "SCALERS   = (\"zscore\", \"robust\")\n",
    "\n",
    "CFG1  = {\"name\": \"1-day\",  \"test_len\": 1,  \"step\": 1}\n",
    "CFG5  = {\"name\": \"5-day\",  \"test_len\": 5,  \"step\": 5}\n",
    "CFG20 = {\"name\": \"20-day\", \"test_len\": 20, \"step\": 20}\n",
    "\n",
    "# Cache settings \n",
    "CACHE_DIR    = Path(\"artifacts\") / ASSET\n",
    "SAVE_FMT     = \"parquet\"   \n",
    "SAVE_RESULTS = True        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Helper: save & load a run\n",
    "\n",
    "def _dump_json(path, obj):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(obj, f, indent=2)\n",
    "\n",
    "def save_run(run_dict, outdir, fmt=\"parquet\", meta=None):\n",
    "    \"\"\"\n",
    "    Persist a run produced by run_cfg.\n",
    "    Saves: per_point, per_window, metrics.json, meta.json\n",
    "    \"\"\"\n",
    "    out = Path(outdir)\n",
    "    out.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # DataFrames\n",
    "    if fmt == \"parquet\":\n",
    "        run_dict[\"per_point\"].to_parquet(out / \"per_point.parquet\")\n",
    "        run_dict[\"per_window\"].to_parquet(out / \"per_window.parquet\")\n",
    "    elif fmt == \"csv\":\n",
    "        run_dict[\"per_point\"].to_csv(out / \"per_point.csv\")\n",
    "        run_dict[\"per_window\"].to_csv(out / \"per_window.csv\")\n",
    "    else:\n",
    "        raise ValueError(\"fmt must be 'parquet' or 'csv'\")\n",
    "\n",
    "    # Metrics\n",
    "    overall = run_dict.get(\"overall\", {})\n",
    "    if hasattr(overall, \"__dict__\"):\n",
    "        metrics = {k: (float(v) if isinstance(v, (int, float)) else v)\n",
    "                   for k, v in overall.__dict__.items()}\n",
    "    elif isinstance(overall, dict):\n",
    "        metrics = overall\n",
    "    else:\n",
    "        metrics = {}\n",
    "    _dump_json(out / \"metrics.json\", metrics)\n",
    "\n",
    "    # Meta\n",
    "    _dump_json(out / \"meta.json\", meta or {})\n",
    "\n",
    "def load_run(indir, fmt=\"parquet\"):\n",
    "    \"\"\"\n",
    "    Reload a saved run and return a dict that mirrors run_cfg output\n",
    "    enough to work with your existing plotting functions.\n",
    "    \"\"\"\n",
    "    p = Path(indir)\n",
    "    if fmt == \"parquet\":\n",
    "        per_point  = pd.read_parquet(p / \"per_point.parquet\")\n",
    "        per_window = pd.read_parquet(p / \"per_window.parquet\")\n",
    "    else:\n",
    "        per_point  = pd.read_csv(p / \"per_point.csv\", index_col=0, parse_dates=True)\n",
    "        per_window = pd.read_csv(p / \"per_window.csv\", index_col=0, parse_dates=True)\n",
    "\n",
    "    try:\n",
    "        with open(p / \"metrics.json\") as f:\n",
    "            metrics = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        metrics = {}\n",
    "\n",
    "    return {\n",
    "        \"per_point\":  per_point,\n",
    "        \"per_window\": per_window,\n",
    "        \"overall\":    SimpleNamespace(**metrics),\n",
    "        \"name\":       p.name\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    asset = load_prices(ASSET, start=START, end=END)\n",
    "    mkt   = load_prices(MARKET, start=START, end=END, columns=(\"Close\",))\n",
    "\n",
    "    \n",
    "    features = build_features(\n",
    "        stock_close=asset[\"Close\"],\n",
    "        stock_volume=asset[\"Volume\"],\n",
    "        market_close=mkt[\"Close\"],\n",
    "        include_eoq=True,\n",
    "        macd=\"hist\",\n",
    "    )\n",
    "\n",
    "    \n",
    "    features = add_lags(features, cols=[\"mkt_ret\",\"vol_z\",\"macd_hist\"], lags=(1,2))\n",
    "\n",
    "    \n",
    "    supervised = make_nextday_target(features, target_col=\"ret\", out_col=TARGET)\n",
    "\n",
    "    \n",
    "    grid_results = []\n",
    "    for exogs in EXOG_SETS:\n",
    "        for sc in SCALERS:\n",
    "            grid_results.append(\n",
    "                run_cfg(\n",
    "                    supervised,\n",
    "                    CFG1,\n",
    "                    scaler=sc,\n",
    "                    exogs=exogs,\n",
    "                    name_suffix=f\"+{','.join(exogs)}\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "    top1 = sorted(grid_results, key=lambda r: r[\"overall\"].rmse)[:3]\n",
    "    print(\"=== CFG1 (1-day) — Top by RMSE ===\")\n",
    "    for r in top1:\n",
    "        m = r[\"overall\"]\n",
    "        print(f\"{r['name']}: RMSE={m.rmse:.6f}  MAE={m.mae:.6f}  Hit={m.hit_rate:.3f}  Windows={m.n_windows}\")\n",
    "\n",
    "    \n",
    "    best1 = top1[0]\n",
    "    plot_hitrate_timeline(best1); plt.show()\n",
    "    plot_cum_pnl(best1[\"per_point\"], f\"Cumulative strategy vs Buy&Hold — {best1['name']}\")\n",
    "\n",
    "    \n",
    "    res1  = run_cfg(supervised, CFG1)\n",
    "    res5  = run_cfg(supervised, CFG5)\n",
    "    res20 = run_cfg(supervised, CFG20)\n",
    "\n",
    "    \n",
    "    print_overall_metrics([res1, res5, res20])\n",
    "\n",
    "    \n",
    "    plot_hitrate_timeline(res1);  plt.show()\n",
    "    plot_hitrate_timeline(res5);  plt.show()\n",
    "    plot_hitrate_timeline(res20); plt.show()\n",
    "\n",
    "    plot_cum_pnl(res1[\"per_point\"],  \"Cumulative strategy vs Buy&Hold - 1-day folds\")\n",
    "    plot_cum_pnl(res5[\"per_point\"],  \"Cumulative strategy vs Buy&Hold — 5-day folds\")\n",
    "    plot_cum_pnl(res20[\"per_point\"], \"Cumulative strategy vs Buy&Hold — 20-day folds\")\n",
    "\n",
    "    \n",
    "    market_features = build_features(\n",
    "        stock_close=mkt[\"Close\"],\n",
    "        stock_volume=pd.Series([1]*len(mkt), index=mkt.index),  \n",
    "        market_close=None,\n",
    "        include_eoq=False,\n",
    "        macd=\"none\",\n",
    "    )\n",
    "    if SAVE_RESULTS:\n",
    "        CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        (CACHE_DIR / \"market_ret.parquet\").write_bytes(\n",
    "            market_features[\"ret\"].to_frame(\"ret\").to_parquet()\n",
    "            if SAVE_FMT == \"parquet\"\n",
    "            else market_features[\"ret\"].to_frame(\"ret\").to_csv().encode()\n",
    "        )\n",
    "\n",
    "    plot_detailed_strategy_window(res1[\"per_point\"], market_data=market_features,\n",
    "                                  title=\"Daily returns inside the window - 1-day strategy\", max_days=30)\n",
    "    plot_detailed_strategy_window(res5[\"per_point\"], market_data=market_features,\n",
    "                                  title=\"Daily returns inside the window - 5-day strategy\", max_days=150)\n",
    "    plot_detailed_strategy_window(res20[\"per_point\"], market_data=market_features,\n",
    "                                  title=\"Daily returns inside the window - 20-day strategy\", max_days=300)\n",
    "\n",
    "    plot_strategy_heatmap(res1[\"per_point\"], \"1-day Strategy Monthly Performance Heatmap\")\n",
    "    plot_strategy_heatmap(res5[\"per_point\"], \"5-day Strategy Performance Heatmap\")\n",
    "    plot_strategy_heatmap(res20[\"per_point\"], \"20-day Strategy Performance Heatmap\")\n",
    "\n",
    "    plot_rolling_performance_metrics(res1[\"per_point\"],  window=60,  title=\"Rolling Performance Metrics (60-day)\")\n",
    "    plot_rolling_performance_metrics(res5[\"per_point\"],  window=150, title=\"Rolling Performance Metrics (150-day)\")\n",
    "    plot_rolling_performance_metrics(res20[\"per_point\"], window=300, title=\"Rolling Performance Metrics (300-day)\")\n",
    "\n",
    "    enhanced_plot_cum_pnl(res1[\"per_point\"],  market_ret_series=market_features.get('ret'),\n",
    "                          title=\"Enhanced Strategy Performance Analysis — 1-day\")\n",
    "    enhanced_plot_cum_pnl(res5[\"per_point\"],  market_ret_series=market_features.get('ret'),\n",
    "                          title=\"Enhanced Strategy Performance Analysis — 5-day\")\n",
    "    enhanced_plot_cum_pnl(res20[\"per_point\"], market_ret_series=market_features.get('ret'),\n",
    "                          title=\"Enhanced Strategy Performance Analysis — 20-day\")\n",
    "\n",
    "    \n",
    "    res_stats_1  = analyze_residuals(res1[\"per_point\"])\n",
    "    res_stats_5  = analyze_residuals(res5[\"per_point\"])\n",
    "    res_stats_20 = analyze_residuals(res20[\"per_point\"])\n",
    "    print(\"\\nResidual diagnostics (1-day): \",  {k: round(v, 6) if isinstance(v, float) else v for k, v in res_stats_1.items()})\n",
    "    print(\"\\nResidual diagnostics (5-day): \",  {k: round(v, 6) if isinstance(v, float) else v for k, v in res_stats_5.items()})\n",
    "    print(\"Residual diagnostics (20-day):\",    {k: round(v, 6) if isinstance(v, float) else v for k, v in res_stats_20.items()})\n",
    "\n",
    "\n",
    "    if SAVE_RESULTS:\n",
    "        stamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        out1  = CACHE_DIR / f\"{stamp}_{CFG1['name']}\"\n",
    "        out5  = CACHE_DIR / f\"{stamp}_{CFG5['name']}\"\n",
    "        out20 = CACHE_DIR / f\"{stamp}_{CFG20['name']}\"\n",
    "\n",
    "        save_run(res1,  out1,  fmt=SAVE_FMT, meta={\"asset\": ASSET, \"cfg\": CFG1,  \"start\": START, \"end\": END})\n",
    "        save_run(res5,  out5,  fmt=SAVE_FMT, meta={\"asset\": ASSET, \"cfg\": CFG5,  \"start\": START, \"end\": END})\n",
    "        save_run(res20, out20, fmt=SAVE_FMT, meta={\"asset\": ASSET, \"cfg\": CFG20, \"start\": START, \"end\": END})\n",
    "\n",
    "        \n",
    "        _dump_json(CACHE_DIR / \"latest.json\", {\"cfg1\": str(out1), \"cfg5\": str(out5), \"cfg20\": str(out20)})\n",
    "        print(f\"\\nSaved caches:\\n 1-day → {out1}\\n 5-day → {out5}\\n 20-day → {out20}\")\n",
    "\n",
    "    try:\n",
    "        with open(CACHE_DIR / \"latest.json\") as f:\n",
    "            latest = json.load(f)\n",
    "        r1_cached  = load_run(latest[\"cfg1\"],  fmt=SAVE_FMT)\n",
    "        r5_cached  = load_run(latest[\"cfg5\"],  fmt=SAVE_FMT)\n",
    "        r20_cached = load_run(latest[\"cfg20\"], fmt=SAVE_FMT)\n",
    "\n",
    "        # Market returns for enhanced plot\n",
    "        mkt_ret_path = CACHE_DIR / \"market_ret.parquet\"\n",
    "        if mkt_ret_path.exists():\n",
    "            mkt_ret = pd.read_parquet(mkt_ret_path)[\"ret\"]\n",
    "        else:\n",
    "            mkt_ret = None  \n",
    "\n",
    "        # Plug cached results straight into your plotters\n",
    "        plot_hitrate_timeline(r1_cached);  plt.show()\n",
    "        plot_hitrate_timeline(r5_cached);  plt.show()\n",
    "        plot_hitrate_timeline(r20_cached); plt.show()\n",
    "\n",
    "        plot_cum_pnl(r1_cached[\"per_point\"],  \"Cumulative strategy (cached) — 1-day\")\n",
    "        plot_cum_pnl(r5_cached[\"per_point\"],  \"Cumulative strategy (cached) — 5-day\")\n",
    "        plot_cum_pnl(r20_cached[\"per_point\"], \"Cumulative strategy (cached) — 20-day\")\n",
    "\n",
    "        enhanced_plot_cum_pnl(r1_cached[\"per_point\"],  market_ret_series=mkt_ret,\n",
    "                              title=\"Enhanced Strategy (cached) — 1-day\")\n",
    "        enhanced_plot_cum_pnl(r5_cached[\"per_point\"],  market_ret_series=mkt_ret,\n",
    "                              title=\"Enhanced Strategy (cached) — 5-day\")\n",
    "        enhanced_plot_cum_pnl(r20_cached[\"per_point\"], market_ret_series=mkt_ret,\n",
    "                              title=\"Enhanced Strategy (cached) — 20-day\")\n",
    "\n",
    "        plot_strategy_heatmap(r1_cached[\"per_point\"],  \"1-day (cached) Monthly Heatmap\")\n",
    "        plot_strategy_heatmap(r5_cached[\"per_point\"],  \"5-day (cached) Monthly Heatmap\")\n",
    "        plot_strategy_heatmap(r20_cached[\"per_point\"], \"20-day (cached) Monthly Heatmap\")\n",
    "\n",
    "        plot_rolling_performance_metrics(r1_cached[\"per_point\"],  window=60,  title=\"Rolling (cached) — 60d\")\n",
    "        plot_rolling_performance_metrics(r5_cached[\"per_point\"],  window=150, title=\"Rolling (cached) — 150d\")\n",
    "        plot_rolling_performance_metrics(r20_cached[\"per_point\"], window=300, title=\"Rolling (cached) — 300d\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
